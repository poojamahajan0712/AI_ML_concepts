{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYeHVgSCeCH2Ktbw0M24jj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojamahajan0712/AI_ML_concepts/blob/main/Quantization/Quantization_NB2_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fBD0cynTY9a"
      },
      "outputs": [],
      "source": [
        "#Reference - https://medium.com/game-of-bits/optimizing-tensorflow-models-using-quantization-fb4d09b46fac\n",
        "# https://ai.google.dev/edge/litert/models/post_training_quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* key idea behind quantization - These techniques aim at providing smaller and faster models while keeping the performance of the models almost similar.\n",
        "* Post-training quantization -  the deep learning model is trained with FP-32 tensors and later converted to INT-8(or float-16) in order to get a smaller and faster model for deployment. it is a bit more stable than quantization aware training and easy to use.\n",
        "* In post-quantization techniques, we train the deep learning model normally and save the weights. These weights are later converted into TFLite format and quantized."
      ],
      "metadata": {
        "id": "7-fs0U9kVhOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "dwhgWsWFVXKM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "digits = load_digits()\n",
        "images = digits['images']\n",
        "labels = digits['target']\n",
        "print (images.shape, labels.shape)\n",
        "\n",
        "#Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, random_state=42)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "print (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "#Encoding Labels\n",
        "def get_encoded_labels(target):\n",
        "    output=np.zeros((len(target),10))\n",
        "    for ix, value in enumerate(target):\n",
        "        output[ix][target[ix]] = 1\n",
        "    return output\n",
        "\n",
        "Y_train = get_encoded_labels(y_train)\n",
        "Y_test = get_encoded_labels(y_test)\n",
        "print (Y_train.shape, Y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVSTt-69WWtf",
        "outputId": "dedd92f5-08ee-4f5f-859c-fada7a2ec88f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 8, 8) (1797,)\n",
            "(1347, 8, 8, 1) (450, 8, 8, 1) (1347,) (450,)\n",
            "(1347, 10) (450, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_layer = Input(shape=(8, 8, 1))\n",
        "layer = Conv2D(64, (3,3), activation='relu')(input_layer)\n",
        "layer = Conv2D(32, (3,3), activation='relu')(layer)\n",
        "layer = Conv2D(32, (3,3), activation='relu')(layer)\n",
        "layer = Flatten()(layer)\n",
        "features = Dense(32, activation='relu')(layer)\n",
        "output = Dense(10, activation='softmax')(features)\n",
        "\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "7yfaHBd-WxUk",
        "outputId": "157cd6b0-e129-4c2b-9fd1-e2af10f5c0c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │          \u001b[38;5;34m18,464\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m9,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m4,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,810\u001b[0m (128.16 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,810</span> (128.16 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,810\u001b[0m (128.16 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,810</span> (128.16 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QddA5yDgZZFB",
        "outputId": "b11cd769-ac8a-4aa2-8242-c2d74920d28c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.4809 - loss: 1.6126 - val_accuracy: 0.8956 - val_loss: 0.3401\n",
            "Epoch 2/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8992 - loss: 0.3236 - val_accuracy: 0.9444 - val_loss: 0.2030\n",
            "Epoch 3/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9302 - loss: 0.2404 - val_accuracy: 0.9578 - val_loss: 0.1474\n",
            "Epoch 4/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9644 - loss: 0.1030 - val_accuracy: 0.9689 - val_loss: 0.1101\n",
            "Epoch 5/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9842 - loss: 0.0522 - val_accuracy: 0.9689 - val_loss: 0.0964\n",
            "Epoch 6/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9856 - loss: 0.0501 - val_accuracy: 0.9644 - val_loss: 0.1172\n",
            "Epoch 7/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9911 - loss: 0.0319 - val_accuracy: 0.9711 - val_loss: 0.0830\n",
            "Epoch 8/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9777 - loss: 0.0476 - val_accuracy: 0.9800 - val_loss: 0.0911\n",
            "Epoch 9/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9946 - loss: 0.0204 - val_accuracy: 0.9756 - val_loss: 0.0726\n",
            "Epoch 10/10\n",
            "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0128 - val_accuracy: 0.9711 - val_loss: 0.0873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cfc0df7ee90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_accuracy(predictions, target):\n",
        "    correct = 0\n",
        "    for ix, pred in enumerate(predictions):\n",
        "        true_value = target[ix]\n",
        "        if pred[true_value] == max(pred):\n",
        "            correct += 1\n",
        "    return correct*100/len(target)\n",
        "predictions = model.predict(X_test)\n",
        "get_test_accuracy(predictions, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnlu66yJZ5HX",
        "outputId": "20a7b75b-cf5d-4bb4-f1c6-5cde8da298d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.11111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"saved_model.keras\")"
      ],
      "metadata": {
        "id": "o-XtJfmVVXgn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post training quantization"
      ],
      "metadata": {
        "id": "_048iylzb6LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dynamic range quantization -\n",
        "* Dynamic range quantization provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration.\n",
        "- The key point here is that you don't need a representative dataset for calibration- This means you can apply dynamic range quantization directly to an already-trained model without needing additional data to fine-tune the quantization process\n",
        "* This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision\n",
        "* To further reduce latency during inference, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations."
      ],
      "metadata": {
        "id": "Sn4HZw3VQpgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- Dynamic Range Quantization:\n",
        "\n",
        "* Precision: Typically uses 8-bit integers.\n",
        "\n",
        "* Calibration: Does not require a representative dataset for calibration.\n",
        "\n",
        "* Use Case: Suitable for models where calibration data is not available or practical.\n",
        "\n",
        "* Performance: Reduces memory usage and speeds up computation without significant loss in accuracy\n",
        "\n",
        "\n",
        "* Quantization Process: During training, the weights are in floating-point format. In dynamic range quantization, these weights are converted to 8-bit integers for storage.\n",
        "\n",
        "* Inference: At runtime, the model converts these 8-bit integers back to floating-point values for computation. This means there's some overhead in converting between formats, but it still offers performance benefits due to reduced memory usage."
      ],
      "metadata": {
        "id": "_vDXNE0kgeOg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWHKxFc-gd0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## converting to tflite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "## applying quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoTlkra2aKul",
        "outputId": "5f8bbda7-40e7-48c9-b9ab-e823829d394e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpu1lz9uma'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 8, 8, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137421982223520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982225984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982226688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982228976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982229504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982231792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982234080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('quantized_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "cgFBdscqS3uR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing with quantised model"
      ],
      "metadata": {
        "id": "IzhUX3bikxWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(input_details)\n",
        "print(output_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjArgAZhTnvU",
        "outputId": "4cf41b44-4e92-46d6-fdf0-bda6f7704135"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'serving_default_keras_tensor:0', 'index': 0, 'shape': array([1, 8, 8, 1], dtype=int32), 'shape_signature': array([-1,  8,  8,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'StatefulPartitionedCall_1:0', 'index': 18, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_details[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqXLq-xvhgcd",
        "outputId": "74b76dd2-6ed1-4a1b-83f2-fe6e2422218d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'serving_default_keras_tensor:0',\n",
              " 'index': 0,\n",
              " 'shape': array([1, 8, 8, 1], dtype=int32),\n",
              " 'shape_signature': array([-1,  8,  8,  1], dtype=int32),\n",
              " 'dtype': numpy.float32,\n",
              " 'quantization': (0.0, 0),\n",
              " 'quantization_parameters': {'scales': array([], dtype=float32),\n",
              "  'zero_points': array([], dtype=int32),\n",
              "  'quantized_dimension': 0},\n",
              " 'sparsity_parameters': {}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# interpreter.set_tensor() is used to set or assign the input data (the image) to the input tensor for the model to process.\n",
        "# After calling set_tensor() to assign the input, you need to invoke the model to run the inference and calculate the outputs.\n",
        "# get_tensor(): This method retrieves the output tensor after the inference is complete.After the model has run the inference, the results are stored in the output tensor, and get_tensor() allows you to access these results.\n",
        "\n",
        "\n",
        "predictions = []\n",
        "for img in X_test:\n",
        "    interpreter.set_tensor(input_details[0]['index'], [img.astype('float32')])\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(output_data[0])\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "get_test_accuracy(predictions, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW3UDlmiU4h6",
        "outputId": "bedc1fda-b09c-4f27-dda7-3996483429b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.11111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparing model size"
      ],
      "metadata": {
        "id": "XJkbXkD_kA_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model_path):\n",
        "  model_size = os.path.getsize(model_path)\n",
        "  model_size_mb = model_size / (1024*1024)\n",
        "  print(f\"model size: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "vIQ2TtsNVVOW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/saved_model.keras\"\n",
        "get_model_size(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Zvs76Ki8Ff",
        "outputId": "73534706-6eb7-4d96-8808-5ff00bd7a863"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 0.41 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/quantized_model.tflite\"\n",
        "get_model_size(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpWiTKr1jU9V",
        "outputId": "e397f1f7-98de-4887-d905-6361fb297dfe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 0.04 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Float-16  Quantization\n",
        "* Float-16 quantization reduces the model-size by converting model weights from FP-32 to FP-16 numbers. This technique reduces the model size to approximately half and results in minimum accuracy loss."
      ],
      "metadata": {
        "id": "FqOGxTjueNpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Precision: Uses 16-bit floating-point numbers.\n",
        "\n",
        "* Calibration: Does not require a representative dataset for calibration.\n",
        "\n",
        "* Use Case: Best for models that will run on hardware with support for 16-bit floating-point operations, such as GPUs.\n",
        "\n",
        "* Performance: Offers a balance between memory usage and computational speed, with less precision loss compared to 8-bit quantization."
      ],
      "metadata": {
        "id": "KQD_5P4Xg0kU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quantization Process: Weights are converted from 32-bit floating-point to 16-bit floating-point format.\n",
        "\n",
        "* Inference: The model uses these 16-bit floating-point values directly for computation. This provides a balance between reducing memory usage and maintaining precision, especially on hardware optimized for floating-point operations like GPUs."
      ],
      "metadata": {
        "id": "rEzlHUA9gyYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_quant_model2 = converter.convert()\n",
        "\n",
        "with open('quantized_model2.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model2)"
      ],
      "metadata": {
        "id": "jXk7rI9ZSElg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db1d420-7ec8-4e75-cb12-b266cd57d9fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp9jszrvot'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 8, 8, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137421982223520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982225984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982226688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982228976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982229504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982231792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982234080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/quantized_model2.tflite\"\n",
        "get_model_size(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phZMUo0VetXp",
        "outputId": "afa99c19-133a-42a4-c4e7-26a2cd33dff1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 0.07 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Full integer quantization\n",
        "\n"
      ],
      "metadata": {
        "id": "G1aVnMNnfRL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Precision: Uses 8-bit integers for both weights and activations.\n",
        "* Calibration: Requires a representative dataset to determine the scale and zero-point values.\n",
        "\n",
        "* Use Case: Ideal for models where you have access to calibration data and want to maximize performance on integer-only hardware.\n",
        "\n",
        "* Performance: Further reduces memory usage and improves latency compared to dynamic range quantization."
      ],
      "metadata": {
        "id": "z0ga3xcMhDZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Quantization Process: Both weights and activations are converted to 8-bit integers.\n",
        "\n",
        "* Inference: During inference, the model uses these 8-bit integers directly for computation. This avoids the overhead of converting back to floating-point values, making the process faster on hardware that supports integer operations.*"
      ],
      "metadata": {
        "id": "da-SVVx2hJdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_calibration_steps=1\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    for _ in range(num_calibration_steps):\n",
        "        input_data = [X_test[:10].astype('float32')]\n",
        "        yield input_data\n",
        "\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_quant_model3 = converter.convert()\n",
        "\n",
        "with open('quantized_model3.tflite', 'wb') as f:\n",
        "    f.write(tflite_quant_model3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XUEFtkde9Nt",
        "outputId": "381b952a-40df-4bc0-dc43-3dfde7a73e76"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpn18u9_2d'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 8, 8, 1), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137421982223520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982225984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982226688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982228976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982229504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982231792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982235664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137421982234080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/quantized_model3.tflite\"\n",
        "get_model_size(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeY9jRrohnee",
        "outputId": "f03a1604-e704-4cad-85f6-a7c27aaeceea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 0.04 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEVY7s4Vh3GO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}